{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:40:21.820356793Z",
     "start_time": "2023-12-08T05:40:20.192075223Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.miai.vn/2020/12/22/bert-series-chuong-2-nghich-mot-chut-voi-hugging-face/\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:40:24.204011829Z",
     "start_time": "2023-12-08T05:40:21.860247081Z"
    }
   },
   "id": "8687ccbfd951e6a8"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu văn =  I want to learn Python\n",
      "Token array =  [101, 1045, 2215, 2000, 4553, 18750, 102]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer câu văn bản\n",
    "sent = \"I want to learn Python\"\n",
    "print(\"Câu văn = \", sent)\n",
    "token = tokenizer.encode(sent)\n",
    "print(\"Token array = \", token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:40:24.212547488Z",
     "start_time": "2023-12-08T05:40:24.205765155Z"
    }
   },
   "id": "3fb15dc42441dcf4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode sentence =  [CLS] i want to learn python [SEP]\n"
     ]
    }
   ],
   "source": [
    "#Decode token\n",
    "d_sent = tokenizer.decode(token)\n",
    "print(\"Decode sentence = \", d_sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:40:24.213975948Z",
     "start_time": "2023-12-08T05:40:24.209941502Z"
    }
   },
   "id": "519ff1335b1d44f6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tensor =  tensor([[  101,  1045,  2215,  2000,  4553, 18750,   102]])\n",
      "Sentence tensor size =  torch.Size([1, 7])\n",
      "Sentence tensor type =  torch.int64\n",
      "attention mask type=  torch.float64\n",
      "attention_mask size =  torch.Size([1, 7])\n",
      "last hidden states size =  torch.Size([1, 7, 768])\n",
      "tensor([[[ 0.2363,  0.2132, -0.0683,  ..., -0.1378,  0.2019,  0.4946],\n",
      "         [ 0.4133,  0.1613, -0.0224,  ..., -0.0159,  0.4814,  0.4306],\n",
      "         [ 0.6542,  0.3478,  0.8414,  ...,  0.0991,  0.0711,  0.2553],\n",
      "         ...,\n",
      "         [ 0.0472,  0.2704,  0.4221,  ..., -0.0792,  0.3993, -0.4490],\n",
      "         [-0.2343, -0.2161, -0.4735,  ...,  0.3925,  0.1962, -0.2317],\n",
      "         [ 0.9054,  0.2575, -0.1419,  ..., -0.0431, -0.8176, -0.3018]]])\n"
     ]
    }
   ],
   "source": [
    "# Trích xuất đặc trưng câu văn\n",
    "# Okie, vậy là chúng ta đã token, số hóa được câu văn rồi. Giờ chúng ta sẽ đưa câu văn vào BERT để trích ra đặc trưng nhé:\n",
    "\n",
    "# Chuyển token array thành tensor\n",
    "sent_tensor = torch.tensor([token])\n",
    "print(\"Sentence tensor = \", sent_tensor)\n",
    "print(\"Sentence tensor size = \", sent_tensor.size())\n",
    "print(\"Sentence tensor type = \", sent_tensor.dtype)\n",
    "\n",
    "\n",
    "# Xây dựng attention_mask, ở đây là mảng toàn 1 vì ta quan tâm đến all các token\n",
    "attention_mask = torch.tensor([np.ones(7)])\n",
    "print(\"attention mask type= \", attention_mask.dtype)\n",
    "print(\"attention_mask size = \", attention_mask.size())\n",
    "\n",
    "# Trích đặc trưng\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids = sent_tensor, attention_mask = attention_mask)\n",
    "\n",
    "# In kích thước và giá trị đặc trưng ra màn hình\n",
    "# print(\"last hidden states type = \", last_hidden_states.dtype)\n",
    "print(\"last hidden states size = \", last_hidden_states[0].size())\n",
    "print(last_hidden_states[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:41:14.178724162Z",
     "start_time": "2023-12-08T05:41:14.127701260Z"
    }
   },
   "id": "71a3e89af9084f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lưu ý\n",
    "- Thứ nhất, trong pytorch ra dùng lệnh .size() để in ra kích thước tensor.\n",
    "- Thứ hai, attention mask là gì? Đó là một ma trận/vector có kích thước bằng với vector token và có giá trị là 0/1 để chỉ ra các token nào Bert cần quan tâm và token nào không cần quan tâm. Trong trường hợp này, chúng ta quan tâm all các token nên để là 1 hết. Khi chúng ta xử lý nhiều câu văn bản có độ dài khác nhau, ta sẽ padding thêm các giá trị đặc biệt, và khi đó ta bảo model là : hãy đừng quan tâm đến các cái tao padding thêm vào bằng cách set các giá trị tương ứng trong attetion mask = 0\n",
    "- Thứ ba, output embedding mà chúng ta mong muốn sẽ có size là (1,7,768) . Điều này có nghĩa là có tất cả 7 embedding vector cho 7 từ trong câu (bao gồm cả 2 cái từ đặc biệt [CLS] và [SEP] đó) và mỗi vector có độ dài 768."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17cc86736d8a85aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
